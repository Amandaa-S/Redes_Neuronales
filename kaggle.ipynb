{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ae437c-2567-4068-8b2d-a494db452b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display as dis\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ee498-a1d2-47ab-af3b-b76e4f9f1cad",
   "metadata": {},
   "source": [
    "### Preparación de data de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1fbd5-a943-42b9-8b6b-cf079eb6755d",
   "metadata": {},
   "source": [
    "El objetivo de esta sección es el de leer los archivos de audio del dataset de entrenamiento y poder estardarizar todos los audios.\n",
    "\n",
    "Esto se hace mediante dos funciones de la libreria librosa. La primera parte consiste en la transformacion del archivo a un arrelo unidimencional, de entre todos los arreglos se elije el de mayor tamaño para realizar una completacion de matrices, es decir, todos los audios son colocados dentro de arreglos unidimencionales deltamaño del audio de mayor variacion, estos arreglos estan compuestos originalmente de solo 0. Con esto rellenamos los audios con silencio hasta completar una duracion.\n",
    "\n",
    "Luego estos arreglos de igual tamaño son transformados a matrices usando la funcion $librosa.feature.melspectrogram$, estas matrices se conocen como espectogramas y son una forma de visualizar en forma de imagen el audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1ae625-705b-45b7-8e17-83d789204da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ad3e74-06f4-4ea5-9af2-a8f3be1e1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xaux = []\n",
    "Xlens = []\n",
    "Xsr = []\n",
    "\n",
    "for idFile in dataset[\"Id\"].values:\n",
    "    y, sr = librosa.load(\"Train/\"+idFile, sr=22050, duration=15)\n",
    "    Xaux.append(y)\n",
    "    Xlens.append(len(y))\n",
    "    Xsr.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73de58e6-d46a-4505-8a58-f40e61377392",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "maxSize = np.max(Xlens)\n",
    "for i, data in enumerate(Xaux):\n",
    "    aux = np.zeros((1,maxSize))\n",
    "    aux[0][:Xlens[i]] =  data\n",
    "    melspect = librosa.feature.melspectrogram(y=aux, sr=sr, n_mels=128)\n",
    "    X.append(melspect.T)\n",
    "    \n",
    "y = dataset[\"Expected\"].values\n",
    "labels_respuestas1 = []\n",
    "labels_respuestas2 = []\n",
    "for i in y:\n",
    "    aux = i.split()\n",
    "    labels_respuestas1.append( int(aux[0]) )\n",
    "    labels_respuestas2.append( int(aux[1])-2 )\n",
    "y1 = np.array( labels_respuestas1 )\n",
    "y2 = np.array( labels_respuestas2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883c1b0a-fba3-4919-ae8f-c4df2987f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trS, X_testS, y_trS, y_testS = train_test_split(X, y1, test_size=0.3, train_size=0.7)\n",
    "X_trS, X_valS, y_trS, y_valS = train_test_split(X_trS, y_trS, test_size=0.1, train_size=0.9)\n",
    "\n",
    "X_trN, X_testN, y_trN, y_testN = train_test_split(X, y2, test_size=0.2, train_size=0.8)\n",
    "X_trN, X_valtN, y_trN, y_valN = train_test_split(X_trN, y_trN, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3df47-5531-487c-9cb5-a852333b7027",
   "metadata": {},
   "source": [
    "### Modelos de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357b5a2-8940-4d62-a5e7-d1f2b48408c8",
   "metadata": {},
   "source": [
    "Para clasificar los audios usaremos dos modelos, uno encargado de clasificar el sexo de la persona que habla en el audio y otro encargado de clasificar la nacionalidad del hablante.\n",
    "\n",
    "El primer modelo, el encargado de clasificar el sexo, es un modelo generico con capas de convolucion. Se decidio utilizar capas de convolucion ya que con el tratamiento de datos estamos trabajando con \"imagenes\", por ello, se considero apropiado su uso. Al final para la capa de salida se decidio utilizar un solo nodo con una funcion de activacion sigmoidal ya que esta red solo clasifica si el hablante es hombre o mujer, es decir, una clasificacion binaria.\n",
    "\n",
    "Para el segundo modelo, volvemos a utilizar capas de convolucion y con una capa de salida con 5 neuronas, las cuales representan las nacionalidades posibles, ya que estamos trabajando con un problema de multiples clases, se decidio cambiar la funcion sigmoidal por la funcion softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c549fbb8-b02e-4335-9b1f-fa9faf641840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 535, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 267, 63, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 265, 61, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 132, 30, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 130, 28, 64)       36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 232960)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 232961    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 288,705\n",
      "Trainable params: 288,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelSexo = models.Sequential()\n",
    "modelSexo.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(X[0].shape[0], 128, 1)))\n",
    "modelSexo.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "modelSexo.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "modelSexo.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "modelSexo.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "modelSexo.add(layers.Flatten())\n",
    "\n",
    "modelSexo.add(layers.Dense(1, activation='sigmoid'))\n",
    "modelSexo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f70d6ff6-2f23-455c-9d54-6c3c7bd204f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_59 (Conv2D)          (None, 535, 126, 32)      320       \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 533, 124, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 133, 31, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 131, 29, 64)       18496     \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 129, 27, 64)       36928     \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 32, 6, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 30, 4, 64)         36928     \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 28, 2, 64)         36928     \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 3584)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 32)                114720    \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,005\n",
      "Trainable params: 256,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelNa = models.Sequential()\n",
    "modelNa.add(layers.Conv2D( 32, (3, 3), activation='relu', input_shape=(X[0].shape[0], 128, 1)) )\n",
    "modelNa.add(layers.Conv2D( 32, (3, 3), activation='relu'))\n",
    "modelNa.add(layers.MaxPooling2D((4, 4)))\n",
    "\n",
    "modelNa.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "modelNa.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "modelNa.add(layers.MaxPooling2D((4, 4)))\n",
    "\n",
    "modelNa.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "modelNa.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "modelNa.add(layers.Flatten())\n",
    "modelNa.add(layers.Dense(32, activation='relu'))\n",
    "modelNa.add(layers.Dense(64, activation='relu'))\n",
    "modelNa.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "modelNa.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344f4f1-6e93-4e95-a19d-6b42d5ec34b8",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fb20a-1c2c-493e-8631-c16d79958a78",
   "metadata": {},
   "source": [
    "##### Clasificacion sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e1ca69-9045-4cb8-a036-2e08b96020bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28/28 [==============================] - 67s 2s/step - loss: 0.9292 - accuracy: 0.7107 - val_loss: 0.3161 - val_accuracy: 0.8673\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 63s 2s/step - loss: 0.1885 - accuracy: 0.9214 - val_loss: 0.0967 - val_accuracy: 0.9898\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 63s 2s/step - loss: 0.1248 - accuracy: 0.9567 - val_loss: 0.2288 - val_accuracy: 0.9388\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 63s 2s/step - loss: 0.0486 - accuracy: 0.9863 - val_loss: 0.0430 - val_accuracy: 0.9796\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 63s 2s/step - loss: 0.0372 - accuracy: 0.9875 - val_loss: 0.2159 - val_accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "modelSexo.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics='accuracy')\n",
    "\n",
    "history1 = modelSexo.fit(np.array(X_trS), y_trS, epochs=5, \n",
    "                    validation_data=(np.array(X_valS), y_valS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937bcb5-57ae-4cf0-9c90-3f1588528d88",
   "metadata": {},
   "source": [
    "##### Clasificacion nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1369729e-4d02-4009-b286-92d0340b52ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "28/28 [==============================] - 127s 4s/step - loss: 1.6408 - accuracy: 0.2096 - val_loss: 1.5971 - val_accuracy: 0.2321\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 118s 4s/step - loss: 1.5922 - accuracy: 0.2668 - val_loss: 1.5929 - val_accuracy: 0.2277\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 1.5622 - accuracy: 0.2814 - val_loss: 1.5757 - val_accuracy: 0.2411\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 118s 4s/step - loss: 1.5200 - accuracy: 0.3206 - val_loss: 1.6659 - val_accuracy: 0.2991\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 121s 4s/step - loss: 1.4519 - accuracy: 0.3688 - val_loss: 1.5423 - val_accuracy: 0.2857\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 1.3438 - accuracy: 0.4215 - val_loss: 1.7045 - val_accuracy: 0.2812\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 1.2284 - accuracy: 0.4832 - val_loss: 1.8351 - val_accuracy: 0.2991\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 1.1529 - accuracy: 0.5235 - val_loss: 1.8363 - val_accuracy: 0.3036\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 119s 4s/step - loss: 1.0714 - accuracy: 0.5908 - val_loss: 1.8571 - val_accuracy: 0.3750\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.9391 - accuracy: 0.6480 - val_loss: 1.9794 - val_accuracy: 0.3795\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.8363 - accuracy: 0.6861 - val_loss: 2.5141 - val_accuracy: 0.3482\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.7000 - accuracy: 0.7455 - val_loss: 2.8047 - val_accuracy: 0.3616\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.6061 - accuracy: 0.7915 - val_loss: 3.3933 - val_accuracy: 0.3616\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 118s 4s/step - loss: 0.5346 - accuracy: 0.8072 - val_loss: 3.4509 - val_accuracy: 0.3705\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 118s 4s/step - loss: 0.4611 - accuracy: 0.8262 - val_loss: 4.1683 - val_accuracy: 0.3393\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.5060 - accuracy: 0.8173 - val_loss: 3.4841 - val_accuracy: 0.3795\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.3823 - accuracy: 0.8621 - val_loss: 4.4936 - val_accuracy: 0.3527\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.2414 - accuracy: 0.9193 - val_loss: 4.8066 - val_accuracy: 0.3839\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.2019 - accuracy: 0.9462 - val_loss: 6.0336 - val_accuracy: 0.3304\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.3906 - accuracy: 0.8778 - val_loss: 4.1080 - val_accuracy: 0.3482\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.2880 - accuracy: 0.9294 - val_loss: 4.4248 - val_accuracy: 0.3438\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 119s 4s/step - loss: 0.1675 - accuracy: 0.9540 - val_loss: 6.1409 - val_accuracy: 0.3795\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 118s 4s/step - loss: 0.1432 - accuracy: 0.9563 - val_loss: 7.3048 - val_accuracy: 0.3482\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.1731 - accuracy: 0.9473 - val_loss: 6.3369 - val_accuracy: 0.4196\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.2047 - accuracy: 0.9406 - val_loss: 5.6078 - val_accuracy: 0.3705\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.0776 - accuracy: 0.9798 - val_loss: 6.3839 - val_accuracy: 0.3482\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.0563 - accuracy: 0.9854 - val_loss: 7.5937 - val_accuracy: 0.3571\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 117s 4s/step - loss: 0.0521 - accuracy: 0.9899 - val_loss: 7.8668 - val_accuracy: 0.3348\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 116s 4s/step - loss: 0.1613 - accuracy: 0.9484 - val_loss: 8.4198 - val_accuracy: 0.3393\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 119s 4s/step - loss: 0.3468 - accuracy: 0.9159 - val_loss: 4.5042 - val_accuracy: 0.3750\n"
     ]
    }
   ],
   "source": [
    "modelNa.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "history2 = modelNa.fit(np.array(X_trN), y_trN, epochs=30, \n",
    "    validation_data=(np.array(X_valtN), y_valN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938b9a0-c59c-40ba-8cdc-49c1ae5e5564",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7b4b7-6223-4beb-8357-e2ce910a686b",
   "metadata": {},
   "source": [
    "En esta sección aplicamos el mismo tratamiento que a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe2ddab1-932f-4378-a011-2c47a0f4b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTest = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c34b2801-8006-44ef-9671-72effc340ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "XauxTest = []\n",
    "XlensTest = []\n",
    "XsrTest = []\n",
    "for idFile in datasetTest[\"Id\"].values:\n",
    "    y, sr = librosa.load(\"Test/\"+idFile, sr=22050, duration=15)\n",
    "    XauxTest.append(y)\n",
    "    XlensTest.append(len(y))\n",
    "    XsrTest.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "638b2e5e-89c6-42e6-9e07-c1232565e5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "XTest = []\n",
    "if np.max(XlensTest) > np.max(Xlens):\n",
    "    maxSize = np.max(XlensTest)\n",
    "else:\n",
    "    maxSize = np.max(Xlens)\n",
    "    \n",
    "for i, data in enumerate(XauxTest):\n",
    "    if i%100==0: print(i)\n",
    "    aux = np.zeros((1,maxSize))\n",
    "    aux[0][:XlensTest[i]] =  data\n",
    "    melspect = librosa.feature.melspectrogram(y=aux, sr=sr, n_mels=128)\n",
    "    XTest.append(melspect.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e75fd7-3669-4dd1-95e9-2dc52256a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 13s 665ms/step\n"
     ]
    }
   ],
   "source": [
    "results1 = modelSexo.predict( np.array(XTest) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12707a0a-d642-4acd-b9d4-cc69ad3528e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 20s 1s/step\n"
     ]
    }
   ],
   "source": [
    "results2 = modelNa.predict( np.array(XTest) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed7e55e1-aba3-4e5a-952d-ffb2b51dc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listPre = []\n",
    "for i in range(len(results1)):\n",
    "    auxindex = 0\n",
    "    maxval = 0\n",
    "    for k,j in enumerate(results2[i]):\n",
    "        if j > maxval:\n",
    "            maxval = j\n",
    "            auxindex = k\n",
    "    \n",
    "    auxText = str( int(np.round(results1[i],1)) ) + \" \" + str( auxindex )\n",
    "    listPre.append(auxText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b614147c-8629-4ce9-af62-f75334967577",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasetTest[\"Id\"].values\n",
    "dt = pd.DataFrame( {\"Expected\":listPre}, index=d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "421a40a7-dfa4-4070-bd8a-404182601b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd95e52-903b-4c0e-86e3-e7b345444f15",
   "metadata": {},
   "source": [
    "### Comentarios finales\n",
    "\n",
    "El camino que se decidio tomar en este desafio fue trabajar con dos redes, una que prediga el sexo de la persona y otra que prediga la nacionalidad.\n",
    "El rendimiento de la red de prediccion del sexo tiene una alta accuracy, en el caso de la red de nacionalidad no se tiene un buen rendimiento, se observa un overfitting de la red, lo cual hace que su prediccion sea pobre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
